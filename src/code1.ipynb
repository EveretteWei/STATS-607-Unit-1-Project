{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db7c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48923314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.541</td>\n",
       "      <td>15</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.546</td>\n",
       "      <td>38</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.341</td>\n",
       "      <td>22</td>\n",
       "      <td>665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.613</td>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1     2     3     4     5    6     7     8     9   ...     48  \\\n",
       "0  0.00  0.0  0.00  0.00  0.00  0.00  0.0  0.00  0.00  0.00  ...  0.000   \n",
       "1  0.00  0.0  0.59  0.11  0.00  0.00  0.0  0.00  0.11  0.23  ...  0.227   \n",
       "2  0.06  0.0  0.40  0.00  0.13  0.13  0.0  0.13  0.00  0.00  ...  0.028   \n",
       "3  0.00  0.0  0.00  0.00  0.00  0.00  0.0  0.00  0.00  0.00  ...  0.000   \n",
       "4  0.00  0.0  0.00  0.00  0.00  0.44  0.0  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "      49     50     51     52   53     54  55   56  57  \n",
       "0  0.610  0.000  0.203  0.000  0.0  2.541  15   61   0  \n",
       "1  0.322  0.113  0.056  0.075  0.0  2.546  38  601   0  \n",
       "2  0.085  0.000  0.000  0.000  0.0  2.341  22  665   0  \n",
       "3  0.000  0.000  0.000  0.000  0.0  1.500   4   24   0  \n",
       "4  0.150  0.000  0.000  0.000  0.0  1.613  11   71   0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datatot = pd.read_csv('/Users/everette/Desktop/STATS 607/Project/Unit 1/607 Unit 1 project/spam-data/spam-train.txt', header=None, sep=',')\n",
    "train_datatot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492b6cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.351</td>\n",
       "      <td>69</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.752</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.666</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.727</td>\n",
       "      <td>11</td>\n",
       "      <td>190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.590</td>\n",
       "      <td>739</td>\n",
       "      <td>2333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0  0.12  0.12  0.24  0.0  1.34  0.12  0.00  0.12  0.00  0.00  ...  0.061   \n",
       "1  0.00  0.00  0.32  0.0  0.64  0.64  0.64  0.32  0.32  0.00  ...  0.000   \n",
       "2  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "3  0.00  0.00  0.40  0.0  0.40  0.20  0.00  0.00  0.00  1.01  ...  0.000   \n",
       "4  0.51  0.43  0.29  0.0  0.14  0.03  0.00  0.18  0.54  0.62  ...  0.012   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.020  0.0  0.041  0.041  0.000  2.351   69   254   1  \n",
       "1  0.156  0.0  0.000  0.156  0.000  1.752   19   149   1  \n",
       "2  0.000  0.0  0.000  0.000  0.000  1.666    6    30   0  \n",
       "3  0.030  0.0  0.000  0.302  0.000  1.727   11   190   1  \n",
       "4  0.078  0.0  0.443  0.510  0.133  6.590  739  2333   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datatot = pd.read_csv('/Users/everette/Desktop/STATS 607/Project/Unit 1/607 Unit 1 project/spam-data/spam-test.txt', header=None, sep=',')\n",
    "test_datatot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f47cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_datatot.iloc[:, :-1]\n",
    "train_labels = train_datatot.iloc[:, -1]\n",
    "test_data = test_datatot.iloc[:, :-1]\n",
    "test_labels = test_datatot.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f991d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data_standardized = scaler.fit_transform(train_data)\n",
    "test_data_standardized = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5bc05",
   "metadata": {},
   "source": [
    "**(a)** Fit a penalized logistic regression model, which seeks to minimize\n",
    "\t\t$$\n",
    "\t\t-\\log \\text{ conditional likelihood } + \\lambda \\theta^T \\theta\n",
    "\t\t$$\n",
    "\t\tThe parameter $\\lambda$ can be chosen by cross-validation. Report the mean error rate on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a7d4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.07010107597000326), np.float64(0.07235984354628422))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Logistic Regression on standardized data\n",
    "Cs=10**np.linspace(-4., 4., 100)\n",
    "model_1 = LogisticRegressionCV(Cs=Cs,\n",
    "                                 cv=5, #cross-validation\n",
    "                                 penalty='l2', #ridge\n",
    "                                 solver='lbfgs',\n",
    "                                 max_iter=1000)\n",
    "\n",
    "model_1.fit(train_data_standardized, train_labels)\n",
    "\n",
    "train_1_pred = model_1.predict(train_data_standardized)\n",
    "test_1_pred = model_1.predict(test_data_standardized)\n",
    "\n",
    "# Mean Error Rate\n",
    "train_1_err = np.mean(train_1_pred != train_labels)\n",
    "test_1_err = np.mean(test_1_pred != test_labels)\n",
    "train_1_err, test_1_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33aa90ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data Preprocessing Dataset  Mean Error Rate\n",
      "0       Standardized   Train         0.070101\n",
      "1       Standardized    Test         0.072360\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Data Preprocessing': ['Standardized', 'Standardized'],\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'Mean Error Rate': [train_1_err, test_1_err]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a457b",
   "metadata": {},
   "source": [
    "**(b)** Write your LDA code to fit the standardized data and the log transformed data. Report your error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1710dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.10172807303553962), np.float64(0.09582790091264667))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# LDA on standardized data\n",
    "lda_1 = LinearDiscriminantAnalysis()\n",
    "lda_1.fit(train_data_standardized, train_labels)\n",
    "\n",
    "train_1_pred_lda = lda_1.predict(train_data_standardized)\n",
    "test_1_pred_lda = lda_1.predict(test_data_standardized)\n",
    "\n",
    "# Mean Error Rate\n",
    "train_1_err_lda = np.mean(train_1_pred_lda != train_labels)\n",
    "test_1_err_lda = np.mean(test_1_pred_lda != test_labels)\n",
    "train_1_err_lda, test_1_err_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781cf341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data Preprocessing  Train Error Rate  Test Error Rate\n",
      "0       Standardized          0.101728         0.095828\n"
     ]
    }
   ],
   "source": [
    "lda_results = pd.DataFrame({\n",
    "    'Data Preprocessing': ['Standardized'],\n",
    "    'Train Error Rate': [train_1_err_lda],\n",
    "    'Test Error Rate': [test_1_err_lda]\n",
    "})\n",
    "\n",
    "print(lda_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9b7e8",
   "metadata": {},
   "source": [
    "**(d)** Perform Kernel logistic regression to fit the standardized data and the log transformed data (You may use a existing package or write your own code). Please consider Gaussian and polynomial kernels to produce the corresponding classifiers. Try a range of tuning parameters and show how such choices affect the behavior of the classifier obtained and the misclassification error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5b696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Logistic Regression\n",
    "# Code by: Moritz Korte-Stapff and Simon Fontaine\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy import optimize\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.optimize import _check_optimize_result\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "def _loss_and_grad(w, K, y, alpha, clip=30):\n",
    "    \"\"\"\n",
    "    Computes the loss and the gradient\n",
    "    The loss is the negative likelihood function\n",
    "    :param w:\n",
    "        array-like of shape (n_samples,)\n",
    "        weights\n",
    "    :param K:\n",
    "        array-like of shape (n_sample, n_samples)\n",
    "        the kernel matrix\n",
    "    :param y:\n",
    "        array-like of shape (n_samples,)\n",
    "        labels\n",
    "    :param alpha:\n",
    "        float\n",
    "        penalty parameter\n",
    "    :return:\n",
    "        out : float\n",
    "            The loss\n",
    "        grad : ndarray of shape (X.shape[0],)\n",
    "            The gradient\n",
    "    \"\"\"\n",
    "    n_samples = K.shape[0]\n",
    "\n",
    "    linear_prediction = K.dot(w)\n",
    "    penalty = (alpha / 2.) * w.T.dot(K).dot(w)\n",
    "\n",
    "    # Loss for kernel logistic regression is the negative likelihood\n",
    "    out = np.sum(-y * linear_prediction + np.log(1 + np.exp(linear_prediction))) + penalty\n",
    "\n",
    "    z = expit(linear_prediction)\n",
    "    z0 = y - z - alpha * w\n",
    "\n",
    "    grad = -K.dot(z0)\n",
    "\n",
    "    return out, grad\n",
    "\n",
    "\n",
    "def _kernel_logistic_regression_path(K, y, max_iter, tol=1e-4, coef=None,\n",
    "                                     solver='lbfgs', check_input=True,\n",
    "                                     C = 1):\n",
    "    \"\"\"\n",
    "    Compute the kernel logistic regression model\n",
    "    :param K:\n",
    "        array-like of shape (n_sample, n_features)\n",
    "        Input pitchfx\n",
    "    :param y:\n",
    "        array-like of shape (n_samples,)\n",
    "        Input pitchfx, target values\n",
    "    :param tol:\n",
    "        float, default = 1e-4\n",
    "        The stopping criterion for the solver\n",
    "    :param coef:\n",
    "        array-like of shape (n_samples,)\n",
    "        Initialisation values of coefficients for the regression\n",
    "    :param solver:\n",
    "        str\n",
    "        The solver to be used\n",
    "    :param check_input:\n",
    "        bool, default = True\n",
    "        Determines whether the input pitchfx should be checked\n",
    "    :return:\n",
    "        w0 : ndarray of shape\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implement\n",
    "    # if check_input:\n",
    "\n",
    "    n_samples, n_features = K.shape\n",
    "    classes = np.unique(y)\n",
    "\n",
    "    if not classes.size == 2:\n",
    "        raise ValueError(\"Only binary Classification.\")\n",
    "\n",
    "    func = _loss_and_grad\n",
    "\n",
    "    if coef is None:\n",
    "        w0 = np.zeros(n_samples, order='F', dtype=K.dtype)\n",
    "    else:\n",
    "        w0 = coef\n",
    "\n",
    "    # TODO: implement other solvers\n",
    "    if solver == 'lbfgs':\n",
    "        opt_res = optimize.minimize(\n",
    "            func, w0, method=\"L-BFGS-B\", jac=True,\n",
    "            args=(K, y, 1. / C, 30),\n",
    "            options={\"gtol\": tol, \"maxiter\": max_iter} # The \"iprint\" option has been removed.\n",
    "        )\n",
    "\n",
    "    n_iter = _check_optimize_result(solver, opt_res, max_iter)\n",
    "\n",
    "    w0, loss = opt_res.x, opt_res.fun\n",
    "\n",
    "    return np.array(w0), n_iter\n",
    "\n",
    "\n",
    "class KernelLogisticRegression(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\" Binary Classifier using Kernel Logistic Regression\n",
    "    ----------\n",
    "    kernel : str, default='rbf_kernel'\n",
    "        Used to determine which kernel function is to be used when generating\n",
    "        the kernel matrix\n",
    "    learning_rate : float, default=1\n",
    "        The learning rate for gradient descent\n",
    "    gamma : float, default = 1\n",
    "        Used during the creation for the Gaussian kernel matrix\n",
    "    C : float, default = 1\n",
    "        The inverse of the penalty parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 kernel='rbf',\n",
    "                 learning_rate=1,\n",
    "                 gamma=1,\n",
    "                 degree=3,\n",
    "                 coef0=1,\n",
    "                 C=1,\n",
    "                 tol=1e-4,\n",
    "                 kernel_params=None,\n",
    "                 max_iter=1000):\n",
    "        self.kernel = kernel\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.kernel_params = kernel_params\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _get_kernel(self, X, Y=None):\n",
    "        if callable(self.kernel):\n",
    "            params = self.kernel_params or {}\n",
    "        else:\n",
    "            params = {\"gamma\": self.gamma,\n",
    "                      \"degree\": self.degree,\n",
    "                      \"coef0\": self.coef0}\n",
    "        return pairwise_kernels(X, Y, metric=self.kernel,\n",
    "                                filter_params=True, **params)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"A reference implementation of a fitting function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        if self.C < 0:\n",
    "            raise ValueError(\"Penalty must be positive\")\n",
    "\n",
    "        # Necessary for prediction\n",
    "        self.X_ = X\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        self.label_encoder_ = LabelBinarizer(neg_label=0, pos_label=1)\n",
    "        y_ = self.label_encoder_.fit_transform(y).reshape((-1))\n",
    "\n",
    "        self.classes_ = self.label_encoder_.classes_\n",
    "        K = self._get_kernel(X)\n",
    "\n",
    "        self.coef_, self.n_iter_ = _kernel_logistic_regression_path(K, y_, tol=self.tol, coef=None,\n",
    "                                     C=self.C, solver='lbfgs', check_input=True,\n",
    "                                     max_iter=self.max_iter)\n",
    "\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "\n",
    "        check_is_fitted(self, [\"X_\", \"coef_\"])\n",
    "\n",
    "        K = self._get_kernel(X, self.X_)\n",
    "\n",
    "        scores = K.dot(self.coef_)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" A reference implementation of a predicting function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns an array of ones.\n",
    "        \"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        indices = (scores > 0).astype(int)\n",
    "\n",
    "        return self.classes_[indices]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Probability estimates.\n",
    "        The returned estimates for all classes are ordered by the\n",
    "        label of classes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Vector to be scored, where `n_samples` is the number of samples and\n",
    "            `n_features` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like of shape (n_samples, n_classes)\n",
    "            Returns the probability of the sample for each class in the model,\n",
    "            where classes are ordered as they are in ``self.classes_``.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        pred_1 = expit(self.decision_function(X).clip(-30, 30)).reshape((-1, 1))\n",
    "\n",
    "        return np.hstack((1.0 - pred_1, pred_1))\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict logarithm of probability estimates.\n",
    "        The returned estimates for all classes are ordered by the\n",
    "        label of classes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Vector to be scored, where `n_samples` is the number of samples and\n",
    "            `n_features` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        T : array-like of shape (n_samples, n_classes)\n",
    "            Returns the log-probability of the sample for each class in the\n",
    "            model, where classes are ordered as they are in ``self.classes_``.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        return np.log(self.predict_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da357576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF kernel, gamma=0.1, C=25.0, Test Error=0.0430\n",
      "RBF kernel, gamma=0.1, C=33.333333333333336, Test Error=0.0424\n",
      "RBF kernel, gamma=0.1, C=41.66666666666667, Test Error=0.0417\n",
      "RBF kernel, gamma=0.1, C=50.0, Test Error=0.0417\n",
      "RBF kernel, gamma=0.1, C=58.333333333333336, Test Error=0.0417\n",
      "RBF kernel, gamma=0.1, C=66.66666666666667, Test Error=0.0430\n",
      "RBF kernel, gamma=0.1, C=75.0, Test Error=0.0424\n",
      "RBF kernel, gamma=0.1, C=83.33333333333334, Test Error=0.0437\n",
      "RBF kernel, gamma=0.1, C=91.66666666666667, Test Error=0.0430\n",
      "RBF kernel, gamma=0.1, C=100.0, Test Error=0.0430\n",
      "RBF kernel, gamma=0.15, C=25.0, Test Error=0.0417\n",
      "RBF kernel, gamma=0.15, C=33.333333333333336, Test Error=0.0430\n",
      "RBF kernel, gamma=0.15, C=41.66666666666667, Test Error=0.0424\n",
      "RBF kernel, gamma=0.15, C=50.0, Test Error=0.0430\n",
      "RBF kernel, gamma=0.15, C=58.333333333333336, Test Error=0.0424\n",
      "RBF kernel, gamma=0.15, C=66.66666666666667, Test Error=0.0424\n",
      "RBF kernel, gamma=0.15, C=75.0, Test Error=0.0424\n",
      "RBF kernel, gamma=0.15, C=83.33333333333334, Test Error=0.0411\n",
      "RBF kernel, gamma=0.15, C=91.66666666666667, Test Error=0.0411\n",
      "RBF kernel, gamma=0.15, C=100.0, Test Error=0.0411\n",
      "RBF kernel, gamma=0.2, C=25.0, Test Error=0.0417\n",
      "RBF kernel, gamma=0.2, C=33.333333333333336, Test Error=0.0411\n",
      "RBF kernel, gamma=0.2, C=41.66666666666667, Test Error=0.0404\n",
      "RBF kernel, gamma=0.2, C=50.0, Test Error=0.0411\n",
      "RBF kernel, gamma=0.2, C=58.333333333333336, Test Error=0.0398\n",
      "RBF kernel, gamma=0.2, C=66.66666666666667, Test Error=0.0398\n",
      "RBF kernel, gamma=0.2, C=75.0, Test Error=0.0385\n",
      "RBF kernel, gamma=0.2, C=83.33333333333334, Test Error=0.0385\n",
      "RBF kernel, gamma=0.2, C=91.66666666666667, Test Error=0.0398\n",
      "RBF kernel, gamma=0.2, C=100.0, Test Error=0.0398\n",
      "RBF kernel, gamma=0.25, C=25.0, Test Error=0.0437\n",
      "RBF kernel, gamma=0.25, C=33.333333333333336, Test Error=0.0430\n",
      "RBF kernel, gamma=0.25, C=41.66666666666667, Test Error=0.0411\n",
      "RBF kernel, gamma=0.25, C=50.0, Test Error=0.0404\n",
      "RBF kernel, gamma=0.25, C=58.333333333333336, Test Error=0.0404\n",
      "RBF kernel, gamma=0.25, C=66.66666666666667, Test Error=0.0404\n",
      "RBF kernel, gamma=0.25, C=75.0, Test Error=0.0411\n",
      "RBF kernel, gamma=0.25, C=83.33333333333334, Test Error=0.0411\n",
      "RBF kernel, gamma=0.25, C=91.66666666666667, Test Error=0.0404\n",
      "RBF kernel, gamma=0.25, C=100.0, Test Error=0.0424\n",
      "RBF kernel, gamma=0.3, C=25.0, Test Error=0.0463\n",
      "RBF kernel, gamma=0.3, C=33.333333333333336, Test Error=0.0430\n",
      "RBF kernel, gamma=0.3, C=41.66666666666667, Test Error=0.0424\n",
      "RBF kernel, gamma=0.3, C=50.0, Test Error=0.0424\n",
      "RBF kernel, gamma=0.3, C=58.333333333333336, Test Error=0.0430\n",
      "RBF kernel, gamma=0.3, C=66.66666666666667, Test Error=0.0430\n",
      "RBF kernel, gamma=0.3, C=75.0, Test Error=0.0430\n",
      "RBF kernel, gamma=0.3, C=83.33333333333334, Test Error=0.0424\n",
      "RBF kernel, gamma=0.3, C=91.66666666666667, Test Error=0.0424\n",
      "RBF kernel, gamma=0.3, C=100.0, Test Error=0.0424\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "gammas = np.linspace(0.1, 0.3, 5)\n",
    "Cs = np.linspace(25, 100, 10)\n",
    "\n",
    "# RBF Kernel on Standardized Data\n",
    "results_std_rbf = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    for C in Cs:\n",
    "        model_rbf = KernelLogisticRegression(kernel='rbf', gamma=gamma, C=C, max_iter=1000)\n",
    "        model_rbf.fit(train_data_standardized, train_labels)\n",
    "        pred_test_std_rbf = model_rbf.predict(test_data_standardized)\n",
    "        test_err_std_rbf = 1 - accuracy_score(test_labels, pred_test_std_rbf)\n",
    "        results_std_rbf.append({'kernel':'rbf', 'gamma':gamma, 'C':C, 'test_error':test_err_std_rbf})\n",
    "        print(f\"RBF kernel, gamma={gamma}, C={C}, Test Error={test_err_std_rbf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd23bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best gamma=0.2, C=75.0, Test Error=0.038\n"
     ]
    }
   ],
   "source": [
    "best_result_std_rbf = min(results_std_rbf, key=lambda x: x['test_error'])\n",
    "print(f\"Best gamma={best_result_std_rbf['gamma']}, C={best_result_std_rbf['C']}, Test Error={best_result_std_rbf['test_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2fc027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial kernel, degree=2, coef0=1, C=0.01, Test Error=0.403\n",
      "Polynomial kernel, degree=2, coef0=1, C=1, Test Error=0.403\n",
      "Polynomial kernel, degree=2, coef0=1, C=100, Test Error=0.403\n",
      "Polynomial kernel, degree=3, coef0=1, C=0.01, Test Error=0.403\n",
      "Polynomial kernel, degree=3, coef0=1, C=1, Test Error=0.403\n",
      "Polynomial kernel, degree=3, coef0=1, C=100, Test Error=0.403\n",
      "Polynomial kernel, degree=4, coef0=1, C=0.01, Test Error=0.403\n",
      "Polynomial kernel, degree=4, coef0=1, C=1, Test Error=0.403\n",
      "Polynomial kernel, degree=4, coef0=1, C=100, Test Error=0.403\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Kernel on Standardized Data\n",
    "\n",
    "results_std_poly = []\n",
    "\n",
    "for degree in [2, 3, 4]:\n",
    "    for C in [0.01, 1, 100]:\n",
    "        model_std_poly = KernelLogisticRegression(\n",
    "            kernel='poly', \n",
    "            coef0=100,  \n",
    "            gamma=1e-10,    \n",
    "            max_iter=1000  \n",
    "        )\n",
    "        model_std_poly.fit(train_data_standardized, train_labels)\n",
    "        pred_test_std_poly = model_std_poly.predict(test_data_standardized)\n",
    "        test_err_std_poly = 1 - accuracy_score(test_labels, pred_test_std_poly)\n",
    "        results_std_poly.append({'kernel':'poly', 'degree':degree, 'coef0':1, 'C':C, 'test_error':test_err_std_poly})\n",
    "        print(f\"Polynomial kernel, degree={degree}, coef0=1, C={C}, Test Error={test_err_std_poly:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8bffcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best degree=2, coef0=1, C=0.01, Test Error=0.403\n"
     ]
    }
   ],
   "source": [
    "best_result_std_poly = min(results_std_poly, key=lambda x: x['test_error'])\n",
    "print(f\"Best degree={best_result_std_poly['degree']}, coef0={best_result_std_poly['coef0']}, C={best_result_std_poly['C']}, Test Error={best_result_std_poly['test_error']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46bddf",
   "metadata": {},
   "source": [
    "(a) Fit CART, Bagging, Random Forest, AdaBoost, and Gradient Boosting. Report their classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5fff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2f396b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'CART': DecisionTreeClassifier(random_state=0),\n",
    "    'Bagging': BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(), \n",
    "        n_estimators=100, random_state=0\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100, random_state=0\n",
    "    ),\n",
    "    'AdaBoost': AdaBoostClassifier(\n",
    "        n_estimators=100, random_state=0\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, random_state=0\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34caae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CART ===\n",
      "Accuracy: 0.9505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9719    0.9443    0.9579       916\n",
      "           1     0.9208    0.9595    0.9398       618\n",
      "\n",
      "    accuracy                         0.9505      1534\n",
      "   macro avg     0.9464    0.9519    0.9488      1534\n",
      "weighted avg     0.9513    0.9505    0.9506      1534\n",
      "\n",
      "\n",
      "=== Bagging ===\n",
      "Accuracy: 0.9654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9757    0.9662    0.9709       916\n",
      "           1     0.9506    0.9644    0.9574       618\n",
      "\n",
      "    accuracy                         0.9654      1534\n",
      "   macro avg     0.9632    0.9653    0.9642      1534\n",
      "weighted avg     0.9656    0.9654    0.9655      1534\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 0.9772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9846    0.9771    0.9808       916\n",
      "           1     0.9664    0.9773    0.9718       618\n",
      "\n",
      "    accuracy                         0.9772      1534\n",
      "   macro avg     0.9755    0.9772    0.9763      1534\n",
      "weighted avg     0.9773    0.9772    0.9772      1534\n",
      "\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9512    0.9585    0.9549       916\n",
      "           1     0.9378    0.9272    0.9325       618\n",
      "\n",
      "    accuracy                         0.9459      1534\n",
      "   macro avg     0.9445    0.9428    0.9437      1534\n",
      "weighted avg     0.9458    0.9459    0.9458      1534\n",
      "\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Accuracy: 0.9654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9655    0.9771    0.9712       916\n",
      "           1     0.9654    0.9482    0.9567       618\n",
      "\n",
      "    accuracy                         0.9654      1534\n",
      "   macro avg     0.9654    0.9626    0.9640      1534\n",
      "weighted avg     0.9654    0.9654    0.9654      1534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, clf in models.items():\n",
    "    clf.fit(train_data_standardized, train_labels)\n",
    "    y_pred = clf.predict(test_data_standardized)\n",
    "    acc = accuracy_score(test_labels, y_pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(test_labels, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ab61b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['CART', 'Bagging', 'Random Forest', 'AdaBoost', 'Gradient Boosting']\n",
    "accuracies = [0.9505, 0.9648, 0.9772, 0.9485, 0.9654]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(models, accuracies)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.90, 1.00)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be934a",
   "metadata": {},
   "source": [
    "(2) Fit a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad8f6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.4614\n",
      "Epoch 2/30, Loss: 0.2620\n",
      "Epoch 3/30, Loss: 0.2137\n",
      "Epoch 4/30, Loss: 0.1888\n",
      "Epoch 5/30, Loss: 0.1727\n",
      "Epoch 6/30, Loss: 0.1609\n",
      "Epoch 7/30, Loss: 0.1525\n",
      "Epoch 8/30, Loss: 0.1441\n",
      "Epoch 9/30, Loss: 0.1384\n",
      "Epoch 10/30, Loss: 0.1311\n",
      "Epoch 11/30, Loss: 0.1258\n",
      "Epoch 12/30, Loss: 0.1211\n",
      "Epoch 13/30, Loss: 0.1161\n",
      "Epoch 14/30, Loss: 0.1125\n",
      "Epoch 15/30, Loss: 0.1086\n",
      "Epoch 16/30, Loss: 0.1049\n",
      "Epoch 17/30, Loss: 0.1006\n",
      "Epoch 18/30, Loss: 0.0972\n",
      "Epoch 19/30, Loss: 0.0938\n",
      "Epoch 20/30, Loss: 0.0910\n",
      "Epoch 21/30, Loss: 0.0881\n",
      "Epoch 22/30, Loss: 0.0860\n",
      "Epoch 23/30, Loss: 0.0824\n",
      "Epoch 24/30, Loss: 0.0803\n",
      "Epoch 25/30, Loss: 0.0809\n",
      "Epoch 26/30, Loss: 0.0763\n",
      "Epoch 27/30, Loss: 0.0740\n",
      "Epoch 28/30, Loss: 0.0731\n",
      "Epoch 29/30, Loss: 0.0708\n",
      "Epoch 30/30, Loss: 0.0688\n",
      "\n",
      "=== Neural Network (PyTorch MLP) ===\n",
      "Accuracy: 0.9615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9653    0.9705    0.9679       916\n",
      "           1     0.9560    0.9482    0.9521       618\n",
      "\n",
      "    accuracy                         0.9615      1534\n",
      "   macro avg     0.9606    0.9594    0.9600      1534\n",
      "weighted avg     0.9615    0.9615    0.9615      1534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "y_train = train_datatot.iloc[:, -1].values.astype(np.int64)\n",
    "y_test  = test_datatot.iloc[:, -1].values.astype(np.int64)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_ds = TensorDataset(\n",
    "    torch.from_numpy(train_data_standardized).float(),\n",
    "    torch.from_numpy(y_train)\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.from_numpy(test_data_standardized).float(),\n",
    "    torch.from_numpy(y_test)\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(input_dim=train_data_standardized.shape[1], hidden_dim=100, output_dim=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 30\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len(train_ds):.4f}\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "y_pred = np.concatenate(all_preds)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Neural Network (PyTorch MLP) ===\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e33ef",
   "metadata": {},
   "source": [
    "(3) Fit SVM and Kernel SVM (with Gaussian and polynomial kernels) and report the classification results. (Try a range of tuning parameters and show how such choices affect the classification results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2732512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear SVM ===\n",
      "Best params: {'C': 70}\n",
      "Test accuracy: 0.9328552803129074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9320    0.9574    0.9445       916\n",
      "           1     0.9342    0.8964    0.9149       618\n",
      "\n",
      "    accuracy                         0.9329      1534\n",
      "   macro avg     0.9331    0.9269    0.9297      1534\n",
      "weighted avg     0.9329    0.9329    0.9326      1534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 3.1 linear kernel SVM\n",
    "param_grid_lin = {'C': [60, 65, 70, 75, 80]}\n",
    "grid_lin = GridSearchCV(\n",
    "    SVC(kernel='linear'), \n",
    "    param_grid_lin, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_lin.fit(train_data_standardized, y_train)\n",
    "print(\"=== Linear SVM ===\")\n",
    "print(\"Best params:\", grid_lin.best_params_)\n",
    "y_pred = grid_lin.predict(test_data_standardized)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f52405cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RBF Kernel SVM ===\n",
      "Best params: {'C': 70, 'gamma': 0.01}\n",
      "Test accuracy: 0.9524119947848761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9460    0.9760    0.9608       916\n",
      "           1     0.9626    0.9175    0.9395       618\n",
      "\n",
      "    accuracy                         0.9524      1534\n",
      "   macro avg     0.9543    0.9467    0.9501      1534\n",
      "weighted avg     0.9527    0.9524    0.9522      1534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2 RBF kernel SVM\n",
    "param_grid_rbf = {\n",
    "    'C':     [10, 30, 50, 70, 100, 150, 200],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "grid_rbf = GridSearchCV(\n",
    "    SVC(kernel='rbf'),\n",
    "    param_grid_rbf,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_rbf.fit(train_data_standardized, y_train)\n",
    "print(\"=== RBF Kernel SVM ===\")\n",
    "print(\"Best params:\", grid_rbf.best_params_)\n",
    "y_pred = grid_rbf.predict(test_data_standardized)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6106bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Polynomial Kernel SVM ===\n",
      "Best params: {'C': 50, 'coef0': 1.0, 'degree': 2, 'gamma': 'scale'}\n",
      "Test accuracy: 0.9504563233376793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9497    0.9683    0.9589       916\n",
      "           1     0.9517    0.9239    0.9376       618\n",
      "\n",
      "    accuracy                         0.9505      1534\n",
      "   macro avg     0.9507    0.9461    0.9483      1534\n",
      "weighted avg     0.9505    0.9505    0.9503      1534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Poly kernel SVM\n",
    "param_grid_poly = {\n",
    "    'C':      [30, 40, 45, 50, 55, 60, 70],\n",
    "    'degree': [2, 3, 4, 5, 6],\n",
    "    'gamma':  ['scale'],  \n",
    "    'coef0':  [0.0, 1.0]  \n",
    "}\n",
    "grid_poly = GridSearchCV(\n",
    "    SVC(kernel='poly'),\n",
    "    param_grid_poly,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_poly.fit(train_data_standardized, y_train)\n",
    "print(\"=== Polynomial Kernel SVM ===\")\n",
    "print(\"Best params:\", grid_poly.best_params_)\n",
    "y_pred = grid_poly.predict(test_data_standardized)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62599cbc",
   "metadata": {},
   "source": [
    "(4) compare with all other methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "607u1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
