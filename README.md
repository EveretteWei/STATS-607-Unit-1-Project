# STATS-607-Unit-1-Project：Spam Classification – Reproducible ML Project

## 1) Problem Overview
This repository implements a classic **binary email spam classification** benchmark. Each email is represented by **57 hand‑crafted features**: 
 - 48 word-frequency percentages (e.g., presence of specific tokens);
 - 6 character-frequency percentages (for a set of punctuation marks); 
 - 3 statistics about sequences of capital letters (average run length, longest run, and total length). 

The corpus contains **4,601 messages** split into a fixed **training set (3,065)** and **test set (1,536)**.  
In this project we **only use the standardized version of the data** (column‑wise mean 0, variance 1) to enable a clean, apples‑to‑apples comparison across models. Our goal is to train a diverse set of supervised classifiers and evaluate their **test accuracy** under the same preprocessing pipeline.

---

## 2) Implemented Models (13 total)
**Linear & Kernel‑based**
- Penalized Logistic Regression
- Linear Discriminant Analysis (LDA)
- Kernel Logistic Regression (Gaussian / RBF)
- Kernel Logistic Regression (Polynomial)

**Tree‑based**
- CART (Decision Tree)
- Bagging
- Random Forest
- AdaBoost
- Gradient Boosting

**Neural Network**
- Feedforward MLP (1–2 hidden layers)

**Support Vector Machines**
- Linear SVM
- SVM (Gaussian / RBF kernel)
- SVM (Polynomial kernel)

All models are trained **only on standardized features** and evaluated on the held‑out test set.


---

## 3) Project Structure
- `data/`
  - `raw/` – immutable source data (read‑only)
  - `processed/` – standardized train/test CSVs produced by the pipeline
- `src/`
  - `pipeline/`
    - `01_preprocess.py` – load raw data and write standardized CSVs to `data/processed/`
  - `analysis/`
    - `02_linear_and_kernel_models.py` – Penalized Logistic Regression, LDA, and Kernel Logistic Regression (RBF/Polynomial)
    - `03_tree_based_models.py` – CART, Bagging, Random Forest, AdaBoost, Gradient Boosting
    - `04_neural_network.py` – feedforward MLP (PyTorch)
    - `05_svm_models.py` – Linear SVM, RBF SVM, Polynomial SVM
    - `06_summary_results.py` – aggregate all model CSVs and produce final summary + figure
    - `klr.py` – custom Kernel Logistic Regression estimator used by `02_*`
- `artifacts/` – intermediate outputs from each model script (CSV)
- `results/`
  - `tables/` – final comparison CSV(s)
  - `figures/` – plots generated by the summary script
- `tests/` – automated tests (`pytest`)
- `run_analysis.py` – single entry point to run the entire pipeline
- `requirements.txt` – Python dependencies
- `README.md` – this document

---

## 4) Getting Started
This project is designed to be fully reproducible with a single command. Follow these instructions to set up the project on any fresh system.

### Prerequisites
- Git
- Python **3.8+**

### Clone the repository
```bash
git clone https://github.com/EveretteWei/STATS-607-Unit-1-Project.git
cd STATS-607-Unit-1-Project
```

### Create and activate a virtual environment
**Windows**
```bash
python -m venv 607u1
607u1\Scripts\activate
```
**macOS / Linux**
```bash
python3 -m venv 607u1
source 607u1/bin/activate
```

### Install dependencies
```bash
pip install -r requirements.txt
```

---

## 5) One‑Command Execution
Run the full end‑to‑end pipeline (standardize data → train all 13 models → aggregate results → plot comparison):

```bash
python run_analysis.py
```

The entry point orchestrates `01_preprocess.py` through `06_summary_results.py` and stops on the first error with a clear message.

---

## 6) Run Analysis Results
After a successful run, the following outputs are produced:

- **Final summary CSV** (sorted by accuracy, descending):  
  `results/tables/final_model_comparison.csv`

- **Performance figure**:  
  `results/figures/model_performance_comparison.png`

The figure is embedded below:

![Model Performance Comparison](results/figures/model_performance_comparison.png)

> Each model group also writes its own intermediate artifacts to `artifacts/`:
> - `linear_model_results.csv`, `tree_model_results.csv`, `nn_model_results.csv`, `svm_model_results.csv`  
>   (each must contain at least: `model_name, accuracy`).

---

## 7) Testing Strategy
We use `pytest` with three layers of checks:
- **Data validation** – processed files exist and have expected shapes/columns
- **Function correctness** – per‑model CSVs contain required columns
- **Pipeline integrity** – one‑command run produces all expected outputs

Run tests via:
```bash
pytest
```

---

## 8) Documentation & Style
- Code docstrings follow **PEP 257** (concise one‑line summary + details, with `Args`/`Returns` where applicable).
- Inline comments are kept minimal and in English.
- Scripts print clear progress messages so the pipeline is easy to audit in logs.

---

## 9) Reproducibility Notes
- Paths are relative to the repository root; missing directories are created automatically.
- Seeds are fixed where applicable; minor variations may occur across library versions.
- Re‑running the pipeline overwrites previous artifacts, keeping the workflow idempotent.

